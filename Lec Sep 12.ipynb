{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "da38e87645b77793"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Limits of Linear Classifiers\n",
    "Single neurons can only separate classes that are linearly separable. \n",
    "XOR is not linearly separable.\n",
    "\n",
    "prooved by Convex Sets,\n",
    "First, Half Spaces are convex sets.\n",
    "Suppose there are feasible \n",
    "\n",
    "Let posi\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2136793a5c7335ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Translation Invariance是指，如果一个模型在一个位置上学习到了一个特征，那么这个特征在其他位置上也是有效的。这个特征不会因为位置的变化而失效。\n",
    "feasible set是指，所有满足约束条件的点的集合。\n",
    "feasible指的是可行的，可行的解。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e62004a89908e5c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Maps\n",
    "Some solutions to the XOR problem are to use multiple neurons, or to use a feature map.\n",
    "We can convert some non-linearly separable problems into linearly separable problems by using a feature map.\n",
    "\n",
    "Variance是指，模型对于数据的敏感程度。如果模型对于数据的变化非常敏感，那么模型的方差就很大。直观上来说，方差就是模型的预测值的变化程度。波动大，方差大。\n",
    "Bias是指，模型对于数据的偏差。如果模型对于数据的偏差很大，那么模型的bias就很大。直观上来说，bias就是模型的预测值和真实值之间的差距。与真实值的偏差大，bias大。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11d2055e3d489c5c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-Layer Perceptrons多层感知机\n",
    "\n",
    "LeNet是一个CNN模型，用于手写数字识别。LeNet是一个多层感知机，包含了卷积层和池化层，特点是使用了Sigmoid激活函数和全连接层。\n",
    "AlexNet是一个CNN模型，用于ImageNet分类。AlexNet是一个多层感知机，包含了卷积层和池化层，特点是使用了ReLU激活函数和Dropout正则化。\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a5704c7b30f71ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In simplist terms, all inputs are connected to all outputs This is called a fully connected network."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7747682bf259dd5c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Activation Functions\n",
    "##### 1. Hyperbolic Tangent (tanh) y = tanh(x) \n",
    "##### 2. ReLU (used in AlexNet) y = max(0, x) [Siplicity, and non-linear]\n",
    "##### 3. Soft ReLU y = log(1 + e^x) \n",
    "##### 4. Leaky ReLU y = max(0.01x, x) [Siplicity, and non-linear and avoids dying ReLU problem]\n",
    "dying ReLU problem is when the ReLU function always outputs 0, which means that the neuron is no longer learning.\n",
    "\n",
    "Why non-linear activation functions? Because if we use a linear activation function, then the whole network would be linear, and we would not be able to learn complex patterns in the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eaa36535b4dfca43"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Learning\n",
    "Neural nets can be viewed as feature learning algorithms    \n",
    "The goal is to learn and make data separable under the basis of the features learned."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2d2db3d2e12fae9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Expressiveness of Neural Networks: Universal Approximation Theorem\n",
    "这是一个数学定理，证明了神经网络可以拟合任何函数。\n",
    "是无限神经元的神经网络可以拟合任何函数还是有限神经元的神经网络可以拟合任何函数？\n",
    "有限神经元的神经网络可以拟合任何函数。\n",
    "\n",
    "Why call deep learning deep? Because it has many layers. 学习Composition of Functions\n",
    "What about wide learning? Wide learning is when you have many neurons in a layer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fcc78d80d675426"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 5. logistic sigmoid y = 1 / (1 + e^-x) [Siplicity, and non-linear]\n",
    "ReLU and Logistic Sigmoid are the most commonly used activation functions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e685dc4e576ee651"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Need to represent an exponential large network.\n",
    "However it is hard to train a deep network because of hardship in calculating the gradient.\n",
    "\n",
    "We need a compact representation of the network.\n",
    "Compact指的是紧凑的，简洁的。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54b8ef6b977c656"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Backpropagation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23f082d00caaa6a8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
